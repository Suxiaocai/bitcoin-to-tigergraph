package main

import (
	"encoding/csv"
	"encoding/hex"
	"flag"
	"fmt"
	"github.com/piotrnar/gocoin/lib/btc"
	"github.com/piotrnar/gocoin/lib/others/blockdb"
	"log"
	"os"
	"strconv"
	"strings"
	"sync"
	"time"
)

// Building structs to hold data
type BlockDat struct {
	dat []byte
	num int
	err  error
}

type TxnDat struct {
	block *btc.Block
	tx    *btc.Tx
}

type File struct {
	fl *os.File
	err error
}

func main() {

	// PARAMETERS
	var batchSize int
	var endBlock int
	var desiredStartBlock int
	var database string
	var outputDir string

	flag.IntVar(&batchSize, "batch", 1000, "size of batch")
	flag.IntVar(&endBlock, "end", 350000, "size of end block")
	flag.IntVar(&desiredStartBlock, "start", 0, "size of desired start block")
	flag.StringVar(&database, "db", "", "database directory")
	flag.StringVar(&outputDir, "output", "./", "output directory")

	flag.Parse()

	// check data directory exist or not
	if _, err := os.Stat(database); os.IsNotExist(err) {
		fmt.Println("Database directory doesn't exist!")
		os.Exit(1)
	}

	// create output directory if necessary
	if _, ok := os.Stat(outputDir); os.IsNotExist(ok) {
		fmt.Println("Creating " + outputDir)
		errMkdir := os.MkdirAll(outputDir, os.ModePerm)
		if errMkdir != nil {
			fmt.Println("Cannot create", outputDir)
			os.Exit(1)
		} 
		fmt.Println(outputDir," created")
	}

	log.Println("Starting the parsing process...")
	// Set real Bitcoin network
	Magic := [4]byte{0xF9, 0xBE, 0xB4, 0xD9}

	// Specify blocks directory
	BlockDatabase := blockdb.NewBlockDB(database, Magic)

	// make map for files
	fileMap := make(map[string] File)

	// make map for csv_writers
	writerMap := make(map[string] *csv.Writer)

	// Preparing files to write to
	for _,v := range []string{"blocks","transactions","input","output"} {
		file, err := os.OpenFile(outputDir+"/"+v+".csv", os.O_CREATE|os.O_APPEND|os.O_WRONLY, 0600)
		fileMap[v] = File{fl:file, err:err}
		checkError("Cannot create file", fileMap[v].err)
		defer fileMap[v].fl.Close()
		writerMap[v] = csv.NewWriter(fileMap[v].fl)
		writerMap[v].Flush()
	}

	// make channels to pass data
	block_channel := make(chan BlockDat, 50000)
	txn_channel := make(chan TxnDat, 300000)

	// Create wait group so that goroutines can all finish before proceeding to next batch
	var tx_wg sync.WaitGroup

	// Process block data in channel in the background
	go func() {
		for msg := range block_channel {
			processBlock(msg, txn_channel, writerMap["blocks"], tx_wg)
		}
	}()

	// Process transaction data in channel in the background
	go func() {
		for msg := range txn_channel {
			processTxn(msg, writerMap["transactions"], writerMap["input"], writerMap["output"])
		}
	}()

	// loop through all the blocks
	for startBlock := 0; startBlock <= endBlock; startBlock++ {

		dat, err := BlockDatabase.FetchNextBlock()

		if startBlock < desiredStartBlock {
			continue
		}

		// Store block data in channel
		block := BlockDat{dat: dat, err: err, num: startBlock}
		block_channel <- block

		// Once batch size is reached process the rest of the txn channel and block channel before proceeding
		if startBlock%batchSize == 0 {
			log.Println("Reached batch:", startBlock/batchSize)
			for len(block_channel) > 0 {
				time.Sleep(10 * time.Millisecond)
			}

			log.Println("Cleaning up transactions...")
			for len(txn_channel) > 0 {
				time.Sleep(10 * time.Millisecond)
			}
		}
	}

	tx_wg.Wait()

	log.Println("Closing Channels...")
	close(block_channel)
	close(txn_channel)

	log.Println("FINISHED!")
}

func checkError(message string, err error) {
	if err != nil {
		log.Fatal(message, err)
	}
}

func processTxn(data TxnDat, txn_writer *csv.Writer, in_writer *csv.Writer, out_writer *csv.Writer) {

	tx := data.tx
	bl := data.block

	txn_msg := fmt.Sprintf("%v,%v,%v,%v,%v,%v",
		tx.Hash.String(),
		tx.Size,
		tx.Lock_time,
		tx.Version,
		bl.Hash,
		tx.IsCoinBase(),
	)

	txn_writer.Write(strings.Split(txn_msg, ","))

	// checking if coins are autogenerated (i.e. no input) or part of a previous output
	if !tx.IsCoinBase() {
		for txin_index, txin := range tx.TxIn {
			prev_outid := strings.Split(txin.Input.String(), "-")
			outid, _ := strconv.Atoi(prev_outid[1])
			txn_outid := prev_outid[0] + "_" + strconv.Itoa(outid)
			input_msg := fmt.Sprintf("%v,%v,%v,%v,%v",
				txn_outid,
				txin_index,
				tx.Hash.String(),
				hex.EncodeToString(txin.ScriptSig),
				txin.Sequence,
			)
			in_writer.Write(strings.Split(input_msg, ","))
		}
	}

	for txo_index, txout := range tx.TxOut {
		curr_outid := tx.Hash.String() + "_" + strconv.Itoa(txo_index)
		txout_addr := btc.NewAddrFromPkScript(txout.Pk_script, false)
		output_msg := fmt.Sprintf("%v,%v,%v,%v,%v",
			curr_outid,
			txo_index,
			txout.Value,
			txout_addr,
			tx.Hash.String(),
		)
		out_writer.Write(strings.Split(output_msg, ","))
	}

}

func processBlock(data BlockDat, txn_channel chan TxnDat, block_writer *csv.Writer, tx_wg sync.WaitGroup) {

	i := data.num
	dat := data.dat
	err := data.err

	if dat == nil || err != nil {
		log.Println("END of DB file")
		return
	}

	bl, err := btc.NewBlock(dat)

	if err != nil {
		println("Block inconsistent:", err.Error())
		return
	}

	block_msg := fmt.Sprintf("%v,%v,%v,%v,%v,%v,%v",
		i,
		bl.Hash,
		hex.EncodeToString(bl.MerkleRoot()),
		reverse_flip_pairs(hex.EncodeToString(bl.ParentHash())),
		convert_to_time(bl.BlockTime()),
		bl.Version(),
		bl.Bits(),
	)

	block_writer.Write(strings.Split(block_msg, ","))

	bl.BuildTxList()
	tx_wg.Add(len(bl.Txs))


	for _, tx := range bl.Txs {
		go func(tx *btc.Tx, bl *btc.Block) {

			defer tx_wg.Done()

			transaction := TxnDat{block: bl, tx: tx}
			txn_channel <- transaction
		}(tx, bl)
	}
}

func reverse_flip_pairs(parent_hash string) string {

	rs := []rune(parent_hash)
	for i, j := 0, len(rs)-1; i < j; i, j = i+1, j-1 {
		rs[i], rs[j] = rs[j], rs[i]
	}
	for i := 1; i < len(rs); i += 2 {
		rs[i], rs[i-1] = rs[i-1], rs[i]
	}
	return string(rs)
}

func convert_to_time(sec_to_add uint32) time.Time {

	start := time.Date(1970, 1, 1, 0, 0, 0, 0, time.UTC)
	secs := strconv.Itoa(int(sec_to_add)) + "s"
	dur, _ := time.ParseDuration(secs)

	return start.Add(dur)
}
